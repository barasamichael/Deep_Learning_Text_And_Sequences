{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Deep Learning for Text and Sequences\n",
    "## Preamble\n",
    "\n",
    "This series explores deep-learning models that can process text, timeseries and sequence data in general.\n",
    "\n",
    "### Fundamental Algorithms\n",
    "- Recurrent Neural Networks\n",
    "- One Dimensional(1D) Convnets\n",
    "\n",
    "### Applications of Deep Learning Algorithms Sequence Processing\n",
    "1. Document Classification and Timeseries Classification\n",
    "    - Identifying the author of an article\n",
    "2. Timeseries Comparisons\n",
    "    - Estimating how close two documents are\n",
    "3. Sequence-to-Sequence Learning\n",
    "    - Decoding and English Statement into Swahili\n",
    "4. Sentiment Analysis\n",
    "    - Classifying the sentiment of movie reviews as +ve or -ve\n",
    "5. Timeseries Forecasting\n",
    "    - Predicting the weather at a certain location, given recent weather data\n",
    "   \n",
    "### Our Area of Focus\n",
    "- Sentiment Analysis of the IMDB Dataset\n",
    "- Weather Temperature Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working With Text Data\n",
    "Text data is understood either as a __sequence of characters__ or a __sequence of words__, with deep-learning models __just mapping the statistical structure of written language without actually understanding it in a human sense__. \n",
    "This is deemed sufficient in most simple textual tasks.\n",
    "\n",
    "Deep-learning for natural-language processing is <span style = \"color: red;\">___pattern recognition applied to words, sentences and paragraphs___</span>.  \n",
    "Deep-learning models work with __numeric tensors only__.\n",
    "\n",
    "### Discover your Vocabulary\n",
    "1. __Vectorizing__: (in context of text) The process of transforming text into numerical tensors through the following ways:\n",
    "    - Segmenting of text into words, and transforming each word into a vector\n",
    "    - Segmenting of text into characters, and transforming each character into \n",
    "        a vector\n",
    "    - Extracting __n-grams__ of words or characters, and transforming each \n",
    "        n-gram into a vector\n",
    "2. __Tokenizing__: The breaking of text into different units known as __tokens__.\n",
    "3. __N-grams__: Overlapping groups of multiple consecutive words or characters.\n",
    "\n",
    "All text-vectorization processes consist of applying some tokenization and then associating numeric tensors with the generated tokens. These vectors, packed into __sequence tensors__, are fed into deep neural networks.  \n",
    "\n",
    "### Ways to Associate a Vector With a Token\n",
    "- One-hot encoding of tokens\n",
    "- Token embedding (typicaly used exclusively for words and called \n",
    "    word tokenization)\n",
    "\n",
    "## Understanding N-grams and Bags-of-words\n",
    "Word n-grams are <span style = \"color: red;\">__groups of N(or fewer) consecutive words or characters that one can extract from a sentence__</span>.\n",
    "\n",
    "### Examples\n",
    "#### Word-based Bigrams (2-grams)\n",
    "{\"The quick\", \"quick brown\", \"brown fox\", \"fox jumps\", \"jumps over\", \"over the\", \"the lazy\", \"lazy dog\"}\n",
    "\n",
    "#### Word-based Bigrams (3-grams)\n",
    "{\"The\", \"quick\", \"brown\", \"The quick\", \"quick brown\", \"brown fox\",\n",
    " \"The quick brown\", \"quick brown fox\", \"brown fox jumps\", \n",
    " \"fox jumps over\", \"jumps over the\", \"over the lazy\", \"the lazy dog\"}\n",
    " \n",
    "The term bag refers to the fact that one's dealing with a set or link of words/tokens rather than a sequence, i.e., they are in no specific order.  \n",
    "The family of tokenization methods is called __bag-of-words__.\n",
    "\n",
    "Bag of words _tends to be used in shallow language-processing models rather\n",
    "than deep learning models_ because it is not an order-preserving tokenization method.\n",
    "\n",
    "Extracting n-grams is a form of feature engineering, and deep learning replaces this _rigid and brittle approach_ with __hierarchical feature engineering__. It is suitable for lightweight, shallow text-processing models such as __logistic regression__ and __random forests__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-Vectorization Processes\n",
    "### One-hot Encoding of Words and Characters\n",
    "It consists of associating a unique integer index with every word and then\n",
    "turning this integer index, _i_ into a binary vector of size _N_ (the size of the vocabulary).  \n",
    "The vector is all zeros except for the _i<sup>th<sup>_ entry, which is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Listing 1-1** Word-level one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T01:54:02.131984Z",
     "iopub.status.busy": "2023-09-23T01:54:02.131548Z",
     "iopub.status.idle": "2023-09-23T01:54:02.149968Z",
     "shell.execute_reply": "2023-09-23T01:54:02.148564Z",
     "shell.execute_reply.started": "2023-09-23T01:54:02.131950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token index:  {'Kenya': 1, 'is': 2, 'a': 3, 'beautiful': 4, 'country': 5, 'To': 6, 'find': 7, 'oneself,': 8, 'you': 9, 'must': 10, 'lose': 11, 'oneself': 12}\n",
      "Initial results array:  [[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "Final results array:  [[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a list of text samples\n",
    "samples = [\"Kenya is a beautiful country\", \n",
    "           \"To find oneself, you must lose oneself\"]\n",
    "\n",
    "# Initialize an empty dictionary to store word indices\n",
    "token_index = {}\n",
    "\n",
    "# Tokenize the text samples and create a dictionary of unique \n",
    "# words with integer indices\n",
    "for sample in samples:\n",
    "    for word in sample.split():\n",
    "         # Check if the word is not already in the dictionary\n",
    "        if word not in token_index:\n",
    "            # Add the word to the dictionary with a unique index\n",
    "            token_index[word] = len(token_index) + 1\n",
    "print(\"Token index: \", token_index)\n",
    "\n",
    "# Define the maximum sequence length\n",
    "max_length = 10\n",
    "# Initialize a NumPy array for one-hot encoding with appropriate dimensions\n",
    "results = np.zeros(\n",
    "    shape = (len(samples), max_length, max(token_index.values()) + 1))\n",
    "print(\"Initial results array: \", results)\n",
    "\n",
    "# Iterate over the samples and their indices\n",
    "for i, sample in enumerate(samples):\n",
    "    # Tokenize the sample into words and limit to the maximum sequence length\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        # Get the index of the word from the dictionary\n",
    "        index = token_index.get(word)\n",
    "        # Set the corresponding element in the results array to 1\n",
    "        results[i, j, index] = 1\n",
    "print(\"Final results array: \", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Listing 1-2.** Character-level one-hot encoding (toy example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T02:00:19.607317Z",
     "iopub.status.busy": "2023-09-23T02:00:19.606878Z",
     "iopub.status.idle": "2023-09-23T02:00:19.618537Z",
     "shell.execute_reply": "2023-09-23T02:00:19.617242Z",
     "shell.execute_reply.started": "2023-09-23T02:00:19.607284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token index dict: \n",
      " {1: '0', 2: '1', 3: '2', 4: '3', 5: '4', 6: '5', 7: '6', 8: '7', 9: '8', 10: '9', 11: 'a', 12: 'b', 13: 'c', 14: 'd', 15: 'e', 16: 'f', 17: 'g', 18: 'h', 19: 'i', 20: 'j', 21: 'k', 22: 'l', 23: 'm', 24: 'n', 25: 'o', 26: 'p', 27: 'q', 28: 'r', 29: 's', 30: 't', 31: 'u', 32: 'v', 33: 'w', 34: 'x', 35: 'y', 36: 'z', 37: 'A', 38: 'B', 39: 'C', 40: 'D', 41: 'E', 42: 'F', 43: 'G', 44: 'H', 45: 'I', 46: 'J', 47: 'K', 48: 'L', 49: 'M', 50: 'N', 51: 'O', 52: 'P', 53: 'Q', 54: 'R', 55: 'S', 56: 'T', 57: 'U', 58: 'V', 59: 'W', 60: 'X', 61: 'Y', 62: 'Z', 63: '!', 64: '\"', 65: '#', 66: '$', 67: '%', 68: '&', 69: \"'\", 70: '(', 71: ')', 72: '*', 73: '+', 74: ',', 75: '-', 76: '.', 77: '/', 78: ':', 79: ';', 80: '<', 81: '=', 82: '>', 83: '?', 84: '@', 85: '[', 86: '\\\\', 87: ']', 88: '^', 89: '_', 90: '`', 91: '{', 92: '|', 93: '}', 94: '~', 95: ' ', 96: '\\t', 97: '\\n', 98: '\\r', 99: '\\x0b', 100: '\\x0c'}\n",
      "Initial results array: \n",
      " [[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n",
      "Final results array: \n",
      " [[[1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "characters = string.printable\n",
    "token_index = dict(zip(range(1, len(characters) + 1), characters))\n",
    "print(\"Token index dict: \\n\", token_index)\n",
    "\n",
    "max_length = 50\n",
    "results = np.zeros((len(samples), max_length, max(token_index.keys()) + 1))\n",
    "print(\"Initial results array: \\n\", results)\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, character in enumerate(sample):\n",
    "        index = token_index.get(character)\n",
    "        results[i, j, index] = 1\n",
    "print(\"Final results array: \\n\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras has inbuilt utilities for doing one-hot encoding of text at the word level or \n",
    "character level, starting from raw text data. They do important features such as \n",
    "stripping special characters from strings and only taking into account the _N_ most \n",
    "common words in the dataset.\n",
    "\n",
    "**Listing 1-3.** Using Keras for word-level one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T02:09:17.670148Z",
     "iopub.status.busy": "2023-09-23T02:09:17.669317Z",
     "iopub.status.idle": "2023-09-23T02:09:17.678776Z",
     "shell.execute_reply": "2023-09-23T02:09:17.677663Z",
     "shell.execute_reply.started": "2023-09-23T02:09:17.670097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Create the tokenizer\n",
    "tokenizer = Tokenizer(num_words = 1000)\n",
    "\n",
    "# Build word indices\n",
    "tokenizer.fit_on_texts(samples)\n",
    "\n",
    "# Convert strings into list of integer indices\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode = 'binary')\n",
    "\n",
    "# Recover word index computed\n",
    "word_index = tokenizer.word_index\n",
    "print(f\"Found {len(word_index)} unique tokens.\")"
   ]
  },
  {
   "attachments": {
    "2d08a494-71b0-4fa4-9be3-d5610d6c0d76.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Word Embeddings\n",
    "![image.png](attachment:2d08a494-71b0-4fa4-9be3-d5610d6c0d76.png)  \n",
    "Whereas the vectors obtained through one-hot encoding are __binary__, __sparse__\n",
    "(mostly made out of 0s) and __high-dimensional__, word embeddings are low dimensional, \n",
    "floating-point numbers (that is, dense vectors as opposed to sparse vectors.)\n",
    "\n",
    "Additionally, unlike word vectors learned via one-hot encoding, word embeddings __are \n",
    "learned from data.__\n",
    "\n",
    "#### Ways of Obtaining Word Embeddings\n",
    "1. Learn word embeddings jointly with the main task one cares about - start with random \n",
    "    word vectors and then learn word vectors in the same way one learns weights of a \n",
    "    neural network.\n",
    "2. Load into the model pretrained word embeddings.\n",
    "\n",
    "#### Learning Word Embeddings Using the Embedding Layer\n",
    "The simplest way to associate a dense vector with a word is to choose the vector at \n",
    "random. A drawback is that __the resulting embedding space has no structure__. For instance, \n",
    "the word _accurately_ and exactly may end up with completely different embedding, despite \n",
    "their interchangeability in most sentences.  \n",
    "It is difficult for a neural network to make sense of such noisy, unstructured embedding \n",
    "space.\n",
    "\n",
    "The geometric relationships between word vectors should reflect the semantic relationships \n",
    "between these words. Word embeddings are meant to map human language into a geometric space.  \n",
    "For instance, in a reasonable embedding space, one would expect:\n",
    "- synonymns to be embedded into similar word vectors\n",
    "- geometric distance (such as L2 distance) between any two word vectors to relate to the \n",
    "    semantic distance between associated words\n",
    "- specific direction in the embedding space to be meaningful\n",
    "\n",
    "What makes a good-embedding space depends heavily on the tasks at hand because _the importance\n",
    " of certain semantic relationships varies from task to task_. Therefore, it is reasonable to \n",
    " learn a new embedding space with every new task. Fortunately, backpropagation makes \n",
    " this easy, and Keras makes it even easier - it's about learning the weights of a layer: \n",
    " __the Embedding layer__.\n",
    " \n",
    "**Listing 1-4.** Instantiating an embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T02:43:51.149978Z",
     "iopub.status.busy": "2023-09-23T02:43:51.149564Z",
     "iopub.status.idle": "2023-09-23T02:43:51.194107Z",
     "shell.execute_reply": "2023-09-23T02:43:51.192544Z",
     "shell.execute_reply.started": "2023-09-23T02:43:51.149947Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(1000, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embedding layer takes at least two arguments:\n",
    "- Number of possible tokens\n",
    "- Dimensionality of the embeddings\n",
    "\n",
    "It is best understood as a dictionary that maps integer indices(which stand for specific \n",
    "words) to dense vectors. It take integers as input, looks up these integers in an internal \n",
    "dictionary and returns the associated vectors. It is effectively a dictionary lookup.\n",
    "\n",
    "The Embedding layer takes as input a __2D tensor of integers__, of __shape(samples, \n",
    "sequence_length)__, where each entry is a sequence of integers.  \n",
    "It can embed sequences of variable lengths with all sequences in a batch having a \n",
    "mandatory equal length because we need to pack them into a single tensor. This can be \n",
    "achieved through padding with zeros for shorter sequences and truncating for longer \n",
    "sequences.\n",
    "\n",
    "The layer returns a __3D floating-point tensor__ of __shape(samples, sequence_length, \n",
    "embedding_dimensionality)__. Such a 3D tensor can then be processed by a __Recurrent \n",
    "Neural Network__ layer of a __One Dimensional(1D) Convolutional__ layer.\n",
    "\n",
    "When one instantiates an Embedding layer, its weights (internal dictionary of token \n",
    "vectors) are initially random akin to any other layer. During training, these word \n",
    "vectors are gradually adjusted via __backpropagation__, structuring the space into\n",
    " something the downstream model can exploit. Once fully trained, the embedding space \n",
    " will show a lot of structure - a kind of structure specialized for the specific \n",
    " problem for which one is training the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
